<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Re          <p>
            RLSF addresses key limitations of traditional reward signals by using symbolic reasoning tools such as:
          </p>
          <p>
            1. <b>Solvers and Provers:</b> Provide formal verification and generate poly-sized certificates (e.g., proofs) that identify errors in model outputs.
          </p>
          <p>
            2. <b>Domain-Specific Tools:</b> Chemistry simulators, compilers, and mathematical verification systems offer token-level guidance for precise error correction.
          </p>
          <p>
            3. <b>Symbolic Validators:</b> Enable non-differentiable feedback without requiring gradient computation, making the approach broadly applicable across domains.
          </p>ntent tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">RLSF: Fine-tuning LLMs via Symbolic Feedback</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://piyush-j.github.io/" target="_blank">Piyush Jha</a></span>
                <span class="author-block">
                  <a href="https://sites.google.com/site/jprithwish" target="_blank">Prithwish Jana</a></span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/pranavkrishnasuresh/" target="_blank">Pranavkrishna Suresh</a></span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/arnav-arora-8762171b4/" target="_blank">Arnav Arora</a></span>
                  <span class="author-block">
                    <a href="https://www.cc.gatech.edu/people/vijay-ganesh" target="_blank">Vijay Ganesh</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Georgia Tech<br>ECAI 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/2405.16661" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2405.16661" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models (LLMs) have transformed AI but often struggle with tasks that require domain-specific reasoning and logical alignment. Traditional fine-tuning methods do not leverage the vast amount of symbolic domain-knowledge available to us via symbolic reasoning tools (e.g., provers), and are further limited by sparse rewards and unreliable reward models. 
            We introduce Reinforcement Learning via Symbolic Feedback (RLSF), a novel fine-tuning paradigm where symbolic reasoning tools (e.g., solvers, provers, and algebra systems) provide fine-grained feedback to LLMs. RLSF uses poly-sized certificates (e.g., proofs) generated by symbolic tools to identify and correct errors in model outputs, offering token-level guidance without requiring differentiable reasoning systems. This paradigm bridges the gap between symbolic reasoning and LLM fine-tuning, enabling precise alignment with domain-specific constraints while addressing key limitations of traditional reward signals. 
            Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on five different applications (that have some associated logical or domain constraints), namely, program synthesis from natural language pseudo-code to programming language, three chemistry tasks, and solving the Game of 24. A key takeaway is that fine-tuning via RLSF enables relatively smaller LLMs to significantly outperform closed-source models that are orders of magnitude larger.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract-->

<!-- A section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Results</h2>
        <div class="content has-text-justified">
          <p>
            Our experiments demonstrate that RLSF consistently outperforms traditional fine-tuning approaches across five different applications with domain-specific constraints, showing that symbolic feedback enables more effective learning than conventional methods.
          </p>
          <ul>
            <li>
              <b>Program Synthesis:</b> RLSF achieves significant improvements in converting natural language pseudo-code to C++ programming language, demonstrating the effectiveness of symbolic feedback from compilers and static analysis tools.
            </li>
            <li>
              <b>Chemistry Tasks:</b> RLSF shows superior performance across three chemistry applications: molecule generation, forward synthesis prediction, and retrosynthesis prediction, leveraging domain-specific chemical reasoning tools for feedback.
            </li>
            <li>
              <b>Game of 24:</b> RLSF enables smaller LLMs to significantly outperform much larger closed-source models, demonstrating the power of symbolic feedback in mathematical reasoning tasks.
            </li>
            <li>
              <b>Model Efficiency:</b> A key finding is that RLSF enables relatively smaller LLMs to achieve performance that <b>significantly exceeds</b> closed-source models that are <b>orders of magnitude larger</b>, highlighting the efficiency gains from symbolic feedback.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">RLSF: Reinforcement Learning via Symbolic Feedback</h2>
        <div class="content has-text-justified">
          <p>
            <b>TLDR:</b> We introduce RLSF, a novel fine-tuning paradigm that leverages symbolic reasoning tools to provide fine-grained feedback to LLMs, enabling precise alignment with domain-specific constraints and significantly improving performance on tasks with logical or mathematical requirements.
          </p>
          <p>
            RLSF addresses key limitations of traditional reward signals by using symbolic reasoning tools such as:
          </p>
          <p>
            1. <b>Solvers and Provers:</b> Provide formal verification and generate poly-sized certificates (e.g., proofs) that identify errors in model outputs.
          </p>
          <p>
            2. <b>Domain-Specific Tools:</b> Chemistry simulators, compilers, and mathematical verification systems offer token-level guidance for precise error correction.
          </p>
          <p>
            3. <b>Symbolic Validators:</b> Enable non-differentiable feedback without requiring gradient computation, making the approach broadly applicable across domains.
          </p>
          <p>
            Unlike traditional RLHF approaches that rely on sparse, scalar rewards from human preferences or simple reward models, RLSF leverages poly-sized certificates generated by symbolic tools to provide <b>token-level feedback</b>. This enables more precise learning by pinpointing specific areas that need improvement rather than providing only binary pass/fail signals.
          </p>
          
        </div>

        <br>

        <h2 class="title is-4">Framework Overview</h2>

        <section class="hero teaser">
          <div class="container is-max-desktop">
            <div class="hero-body">
              <img src="static/images/RLSFvRLHF.png" alt="Contrasting RLHF with RLSF" style="max-width: 80%; display: block; margin: auto;"/>
              <h2 class="subtitle has-text-justified">
                Contrasting RLHF with RLSF: The image depicts two distinct fine-tuning paradigms. (Top) RLHF operates within an environment governed by a black-box reward model, typically offering scalar feedback. (Bottom) By contrast, the environment in RLSF leverages sound symbolic reasoning tools and also provides token-level feedback that is, in turn, based on poly-sized certificates produced by these symbolic tools.
              </h2>
            </div>
          </div>
        </section>

        <section class="hero teaser">
          <div class="container is-max-desktop">
            <div class="hero-body">
              <img src="static/images/Chem.png" alt="RLSF for Chemistry Tasks - Molecule Generation" style="max-width: 80%; display: block; margin: auto;"/>
              <h2 class="subtitle has-text-justified">
                RLSF for one of the chemistry tasks - Molecule Generation: In this illustration, the symbolic environment utilizes RDKit to generate a token-level reward vector as feedback based on the presence or absence of any syntactical errors. Moreover, for the semantic errors, we again use RDKit to check for the presence of the required functional groups mentioned in the input natural language description and penalize the entire generated molecule if it lacks the required functional groups. Each element in the reward vector corresponds to a token in the response, where erroneous tokens are penalized with a value of 0 and correct ones are assigned 1. The last element of the reward vector (corresponding to the &lt;EOS&gt; token) is 1 only if the entire response is correct, otherwise, it is 0.
              </h2>
            </div>
          </div>
        </section>

        <div class="content has-text-justified">
          <p>
            RLSF bridges the gap between symbolic reasoning and LLM fine-tuning by using external symbolic tools to generate 
            rich feedback signals. Unlike traditional RLHF approaches that rely on human preferences or simple reward models, 
            RLSF leverages the vast amount of symbolic domain knowledge available through reasoning tools.
          </p>
          <p>
            The key innovation is the use of poly-sized certificates—formal proofs or verification results—that provide 
            detailed information about where and why errors occur in model outputs. This enables token-level corrections 
            and more efficient learning compared to sparse reward signals.
          </p>
          <p>
            Our extensive evaluations across five different applications demonstrate that RLSF enables smaller LLMs to 
            outperform much larger closed-source models, highlighting the effectiveness of symbolic feedback in guiding 
            the learning process toward domain-specific constraints and logical consistency.
          </p>
        </div>        <br>

</div>
    </div>
    </div>
</section>
<!-- End a section -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@article{jha2024rlsf,
  title={RLSF: Fine-tuning LLMs via Symbolic Feedback},
  author={Jha, Piyush and Jana, Prithwish and Suresh, Pranavkrishna and Arora, Arnav and Ganesh, Vijay},
  journal={28th European Conference on Artificial Intelligence; arXiv preprint arXiv:2405.16661},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
